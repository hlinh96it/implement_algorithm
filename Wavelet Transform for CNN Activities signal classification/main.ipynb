{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 3)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the UCI-HAR time-series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signals_ucihar(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_labels_ucihar(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(int, activities))\n",
    "    return activities\n",
    "\n",
    "\n",
    "def load_ucihar_data(folder):\n",
    "    train_folder = folder + 'train/Inertial Signals/'\n",
    "    test_folder = folder + 'test/Inertial Signals/'\n",
    "    labelfile_train = folder + 'train/y_train.txt'\n",
    "    labelfile_test = folder + 'test/y_test.txt'\n",
    "    train_signals, test_signals = [], []\n",
    "\n",
    "    for input_file in os.listdir(train_folder):\n",
    "        signal = read_signals_ucihar(train_folder + input_file)\n",
    "        train_signals.append(signal)\n",
    "    train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "\n",
    "    for input_file in os.listdir(test_folder):\n",
    "        signal = read_signals_ucihar(test_folder + input_file)\n",
    "        test_signals.append(signal)\n",
    "    test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "    train_labels = read_labels_ucihar(labelfile_train)\n",
    "    test_labels = read_labels_ucihar(labelfile_test)\n",
    "\n",
    "    return train_signals, train_labels, test_signals, test_labels\n",
    "\n",
    "folder_ucihar = 'UCI HAR Dataset/'\n",
    "train_signals_ucihar, train_labels_ucihar, test_signals_ucihar, test_labels_ucihar = \\\n",
    "    load_ucihar_data(folder_ucihar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the WT on the dataset and transforming the data to the right format\n",
    "Bởi vì mỗi signal gồm 9 components khác nhau (coi như 9 sensors), câu hỏi là làm sao ta cho 9 ảnh này vào CNN\n",
    "\n",
    "1. Không thể train separately vì 9 data này có liên quan đến nhau\n",
    "\n",
    "2. Không thể nối vào hết với nhau vì đoạn nối sẽ là điểm coi như bị noise\n",
    "\n",
    "3. Có thể kết hợp 9 ảnh thành 1 ảnh với 9 channels, cũng giống như ảnh thường có 3 channels, CNN có thể làm được điều này\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img width=\"70%\" src=\"https://ataspinar.com/wp-content/uploads/2018/12/9layer_image_CNN.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = range(1,128)\n",
    "waveletname = 'morl'\n",
    "train_size = 50\n",
    "test_size= 5\n",
    "\n",
    "train_data_cwt = np.ndarray(shape=(train_size, 127, 127, 9))  # creat a training matrix\n",
    "\n",
    "for ii in range(0, train_size):\n",
    "    for jj in range(0, 9):\n",
    "        signal = train_signals_ucihar[ii, :, jj]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:127]\n",
    "        train_data_cwt[ii, :, :, jj] = coeff_\n",
    "\n",
    "test_data_cwt = np.ndarray(shape=(test_size, 127, 127, 9))\n",
    "for ii in range(0,test_size):\n",
    "    for jj in range(0,9):\n",
    "        signal = test_signals_ucihar[ii, :, jj]\n",
    "        coeff, freq = pywt.cwt(signal, scales, waveletname, 1)\n",
    "        coeff_ = coeff[:,:127]\n",
    "        test_data_cwt[ii, :, :, jj] = coeff_\n",
    "\n",
    "uci_har_labels_train = list(map(lambda x: int(x) - 1, train_labels_ucihar))\n",
    "uci_har_labels_test = list(map(lambda x: int(x) - 1, test_labels_ucihar))\n",
    "\n",
    "x_train = train_data_cwt\n",
    "y_train = list(uci_har_labels_train[:train_size])\n",
    "x_test = test_data_cwt\n",
    "y_test = list(uci_har_labels_test[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 127, 127, 9), (50, 7))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 23:11:25.254336: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 213ms/step - loss: 3.4072 - accuracy: 0.3200 - val_loss: 12.4355 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 47.5377 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 143ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 67.1559 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 146ms/step - loss: 2.2912e-06 - accuracy: 1.0000 - val_loss: 82.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 94.1106 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 145ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 102.5172 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 108.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 112.8127 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 151ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 115.8013 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 147ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 117.8794 - val_accuracy: 0.0000e+00\n",
      "Train loss: 0.0, Train accuracy: 1.0\n",
      "Test loss: 117.87937927246094, Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "history = History()\n",
    "\n",
    "img_x = 127\n",
    "img_y = 127\n",
    "img_z = 9\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history])\n",
    "\n",
    "train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
